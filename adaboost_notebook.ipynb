{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L3C5joWKWki0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3C5joWKWki0",
        "outputId": "b29d7281-7741-4d91-d987-94bfda293ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.2/863.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.6/413.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install -U -qqq hiplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PuB45q4Bu8Du",
      "metadata": {
        "id": "PuB45q4Bu8Du"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_file = zipfile.ZipFile(\"/content/DataSet.zip\")\n",
        "zip_file.extractall(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hUt5Ar0tu8Dv",
      "metadata": {
        "id": "hUt5Ar0tu8Dv"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = '/content/train/train.csv'\n",
        "SAMPLE_SUB_PATH = '/content/sample_submission.csv'\n",
        "TEST_DIR = '/content/test'          # Folder with 10 TEST_*.csv files\n",
        "OUT_PATH = '/content/submission.csv'\n",
        "VALID_DAYS = 35"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[I 2025-08-23 17:53:47,795] Trial 12 finished with value: 15.279699298235053 and parameters: {'max_depth': 9, 'min_samples_leaf': 11, 'n_estimators': 191, 'learning_rate': 0.0227700153996597, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n"
      ],
      "metadata": {
        "id": "vbG6-eXrdgDx"
      },
      "id": "vbG6-eXrdgDx"
    },
    {
      "cell_type": "markdown",
      "id": "HdfBayL9u8Dw",
      "metadata": {
        "id": "HdfBayL9u8Dw"
      },
      "source": [
        "## 4) Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D0hCnM_vu8Dv",
      "metadata": {
        "id": "D0hCnM_vu8Dv"
      },
      "outputs": [],
      "source": [
        "import os, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from datetime import timedelta\n",
        "#import optuna\n",
        "\n",
        "HORIZON = 7\n",
        "LAGS = 28\n",
        "\n",
        "NUMERIC_FEATS_DEFAULT = [\n",
        "    'is_weekend', 'month', 'weekday','rolling_mean_3',\n",
        "   'rolling_sum_28', 'season_cos', 'season_sin',\n",
        "]\n",
        "\n",
        "def ensure_calendar_feats(df):\n",
        "    if 'date' not in df.columns and '영업일자' in df.columns:\n",
        "        df = df.rename(columns={'영업일자':'date'})\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "    df['weekday'] = df['date'].dt.weekday\n",
        "    if 'is_weekend' not in df.columns:\n",
        "        df['is_weekend'] = (df['date'].dt.weekday >= 5).astype(int)\n",
        "    #if 'dayofmonth' not in df.columns:\n",
        "        #df['dayofmonth'] = df['date'].dt.day\n",
        "    if 'month' not in df.columns:\n",
        "        df['month'] = df['date'].dt.month\n",
        "    if 'rolling_mean_3' not in df.columns or 'rolling_sum_14' not in df.columns:\n",
        "        df = df.sort_values(['영업장명_메뉴명','date'])\n",
        "        df['rolling_mean_3'] = df.groupby('영업장명_메뉴명')['매출수량']\\\n",
        "                                  .transform(lambda s: s.rolling(window=3, min_periods=1).mean())\n",
        "        df['rolling_sum_28'] = df.groupby('영업장명_메뉴명')['매출수량']\\\n",
        "                                  .transform(lambda s: s.rolling(window=14, min_periods=1).sum())\n",
        "        df['season_cos'] = np.cos(df['date'].dt.month)\n",
        "        df['season_sin'] = np.sin(df['date'].dt.month)\n",
        "\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_lags(df, lags=LAGS):\n",
        "    df = df.sort_values(['영업장명_메뉴명','date'])\n",
        "    for i in range(1, lags+1):\n",
        "        df[f'lag_{i}'] = df.groupby('영업장명_메뉴명')['매출수량'].shift(i)\n",
        "    return df\n",
        "\n",
        "def build_supervised(df):\n",
        "    df = df.copy()\n",
        "    if 'date' not in df.columns and '영업일자' in df.columns:\n",
        "        df = df.rename(columns={'영업일자':'date'})\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    df = ensure_calendar_feats(df)\n",
        "    df = add_lags(df, LAGS)\n",
        "    df = df.dropna(subset=[f'lag_{i}' for i in range(1, LAGS+1)])\n",
        "\n",
        "    feats, targets, ref_dates = [], [], []\n",
        "    num_cols = [c for c in df.columns if c.startswith('lag_')] + [c for c in NUMERIC_FEATS_DEFAULT if c in df.columns]\n",
        "\n",
        "    feat_cols = num_cols\n",
        "\n",
        "    df = df.sort_values(['영업장명_메뉴명','date'])\n",
        "    for key, g in df.groupby('영업장명_메뉴명', sort=False):\n",
        "        g = g.reset_index(drop=True)\n",
        "        for t in range(len(g) - HORIZON):\n",
        "            row = g.iloc[t]\n",
        "            y = g['매출수량'].iloc[t+1:t+1+HORIZON].to_numpy(dtype=np.float32)\n",
        "            feats.append(row[feat_cols].to_numpy(dtype=np.float32))\n",
        "            targets.append(y)\n",
        "            ref_dates.append(row['date'])\n",
        "\n",
        "    X = np.vstack(feats) if len(feats) else np.empty((0, len(feat_cols)), dtype=np.float32)\n",
        "    y = np.vstack(targets) if len(targets) else np.empty((0, HORIZON), dtype=np.float32)\n",
        "    ref_dates = pd.to_datetime(pd.Series(ref_dates, name='ref_date'))\n",
        "    return X, y, ref_dates, feat_cols\n",
        "\n",
        "def time_based_split(ref_dates, valid_days=35):\n",
        "    cutoff = ref_dates.max() - pd.Timedelta(days=valid_days)\n",
        "    trn_idx = ref_dates <= cutoff\n",
        "    val_idx = ref_dates > cutoff\n",
        "    return trn_idx.values, val_idx.values\n",
        "\n",
        "def rmse(a, b):\n",
        "    return float(np.sqrt(np.mean((a - b)**2)))\n",
        "\n",
        "\n",
        "\n",
        "def train_adaboost(Xtr, ytr, seed=42):\n",
        "    # AdaBoost requires a base estimator. DecisionTreeRegressor is a common choice.\n",
        "    # We wrap it in MultiOutputRegressor to handle the multi-step forecast (HORIZON=7).\n",
        "    base_estimator = DecisionTreeRegressor(max_depth=9, min_samples_leaf = 11, random_state = 42)\n",
        "\n",
        "    model = MultiOutputRegressor(\n",
        "        AdaBoostRegressor(\n",
        "            estimator=base_estimator,\n",
        "            n_estimators=191,         # Number of boosting stages\n",
        "            learning_rate= 0.0227700153996597,        # Shrinks the contribution of each regressor\n",
        "            loss='square',\n",
        "            random_state=seed\n",
        "        )\n",
        "    )\n",
        "\n",
        "    model.fit(Xtr, ytr)\n",
        "    return model\n",
        "\n",
        "def make_test_features(test_df, feat_cols):\n",
        "    df = test_df.copy()\n",
        "    if 'date' not in df.columns and '영업일자' in df.columns:\n",
        "        df = df.rename(columns={'영업일자':'date'})\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = ensure_calendar_feats(df)\n",
        "\n",
        "    rows, order = [], []\n",
        "    for key, g in df.groupby('영업장명_메뉴명', sort=False):\n",
        "        g = g.sort_values('date')\n",
        "        vals = g['매출수량'].tail(LAGS).to_numpy()\n",
        "        if len(vals) < LAGS:\n",
        "            raise ValueError(f'{key}: need {LAGS} days, got {len(vals)}')\n",
        "        last_row = g.iloc[-1]\n",
        "        named = {f'lag_{i}': vals[-i] for i in range(1, LAGS+1)}\n",
        "        named.update({\n",
        "            'is_weekend': last_row['is_weekend'],\n",
        "            #'dayofmonth': last_row['dayofmonth'],\n",
        "            'weekday' : last_row['weekday'],\n",
        "            'month': last_row['month'],\n",
        "            'rolling_mean_3': last_row['rolling_mean_3'],\n",
        "            'rolling_sum_28': last_row['rolling_sum_28'],\n",
        "            'season_cos' : last_row['season_cos'],\n",
        "            'season_sin' : last_row['season_sin'],\n",
        "\n",
        "        })\n",
        "\n",
        "        feat_vec = [named[c] for c in feat_cols]\n",
        "        rows.append(feat_vec); order.append(key)\n",
        "    return np.array(rows, dtype=np.float32), order\n",
        "\n",
        "def predict_one_test(model, scaler, feat_cols, scaling_cols_indices, non_scaling_cols_indices, test_csv_path):\n",
        "    test_df = pd.read_csv(test_csv_path)\n",
        "    Xtest, order = make_test_features(test_df, feat_cols)\n",
        "\n",
        "    # Apply selective scaling\n",
        "    Xtest_to_scale = Xtest[:, scaling_cols_indices]\n",
        "    Xtest_no_scale = Xtest[:, non_scaling_cols_indices]\n",
        "    Xtest_scaled = scaler.transform(Xtest_to_scale)\n",
        "    Xtest_s = np.concatenate([Xtest_scaled, Xtest_no_scale], axis=1)\n",
        "\n",
        "    yhat = model.predict(Xtest_s)\n",
        "    filename = os.path.basename(test_csv_path)\n",
        "    prefix = re.search(r'(TEST_\\d+)', filename).group(1)\n",
        "    out_rows = []\n",
        "    for j, menu in enumerate(order):\n",
        "        for day in range(1, HORIZON+1):\n",
        "            out_rows.append({\n",
        "                '영업일자': f'{prefix}+{day}일',\n",
        "                '영업장명_메뉴명': menu,\n",
        "                '매출수량': max(0, int(round(float(yhat[j, day-1]))))\n",
        "            })\n",
        "    return pd.DataFrame(out_rows)\n",
        "\n",
        "def convert_to_submission_format(pred_df, sample_submission):\n",
        "    pred_dict = dict(zip(zip(pred_df['영업일자'], pred_df['영업장명_메뉴명']), pred_df['매출수량']))\n",
        "    final_df = sample_submission.copy()\n",
        "    for row_idx in final_df.index:\n",
        "        date = final_df.loc[row_idx, '영업일자']\n",
        "        for col in final_df.columns[1:]:\n",
        "            final_df.loc[row_idx, col] = pred_dict.get((date, col), 0)\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rj9VIu7-u8Dx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj9VIu7-u8Dx",
        "outputId": "9034eea0-0cc6-4b9b-ca05-c078f6f4b2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training AdaBoost Regressor...\n",
            "Training complete.\n",
            "[Validation RMSE] 15.3142\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "if '영업일자' in train.columns and 'date' not in train.columns:\n",
        "    train = train.rename(columns={'영업일자':'date'})\n",
        "train['date'] = pd.to_datetime(train['date'])\n",
        "\n",
        "# Build supervised learning dataset\n",
        "X, y, ref_dates, feat_cols = build_supervised(train)\n",
        "trn_idx, val_idx = time_based_split(ref_dates, valid_days=VALID_DAYS)\n",
        "Xtr, ytr = X[trn_idx], y[trn_idx]\n",
        "Xva, yva = X[val_idx], y[val_idx]\n",
        "\n",
        "# --- Selective Scaling ---\n",
        "non_scaling_cols = ['season_cos', 'season_sin']\n",
        "scaling_cols = [c for c in feat_cols if c not in non_scaling_cols]\n",
        "\n",
        "# Get indices for splitting the arrays\n",
        "scaling_cols_indices = [feat_cols.index(c) for c in scaling_cols]\n",
        "non_scaling_cols_indices = [feat_cols.index(c) for c in non_scaling_cols]\n",
        "\n",
        "# Separate data for scaling\n",
        "Xtr_to_scale = Xtr[:, scaling_cols_indices]\n",
        "Xtr_no_scale = Xtr[:, non_scaling_cols_indices]\n",
        "Xva_to_scale = Xva[:, scaling_cols_indices]\n",
        "Xva_no_scale = Xva[:, non_scaling_cols_indices]\n",
        "\n",
        "# Fit scaler ONLY on the columns to be scaled from the training set\n",
        "scaler = MinMaxScaler().fit(Xtr_to_scale)\n",
        "\n",
        "# Transform and concatenate\n",
        "Xtr_scaled = scaler.transform(Xtr_to_scale)\n",
        "Xtr_s = np.concatenate([Xtr_scaled, Xtr_no_scale], axis=1)\n",
        "\n",
        "Xva_scaled = scaler.transform(Xva_to_scale) if len(Xva) > 0 else None\n",
        "Xva_s = np.concatenate([Xva_scaled, Xva_no_scale], axis=1) if Xva_scaled is not None else None\n",
        "\n",
        "# Train AdaBoost model\n",
        "print(\"Training AdaBoost Regressor...\")\n",
        "model = train_adaboost(Xtr_s, ytr)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "#Evaluate\n",
        "if Xva_s is not None and len(Xva_s):\n",
        "    yhat = model.predict(Xva_s)\n",
        "    val_rmse = rmse(yva, yhat)\n",
        "    print(f'[Validation RMSE] {val_rmse:.4f}')\n",
        "else:\n",
        "    print('Validation set is empty (check VALID_DAYS).')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rolling mean(o):15.2867\n",
        "\n",
        "rolling mean(x): 15.3342"
      ],
      "metadata": {
        "id": "YzWQizK2CDEL"
      },
      "id": "YzWQizK2CDEL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "rolling sum 28 : 15.3198\n",
        "\n",
        "day of month(X) : 15.2736"
      ],
      "metadata": {
        "id": "6jhkdictl4j1"
      },
      "id": "6jhkdictl4j1"
    },
    {
      "cell_type": "markdown",
      "id": "C_xT8AFiu8Dy",
      "metadata": {
        "id": "C_xT8AFiu8Dy"
      },
      "source": [
        "## 5) Inference & Submission File Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b06kqIu8Dy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02b06kqIu8Dy",
        "outputId": "33c50759-e884-4e1e-ff46-f12bc950fb94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found test files: 10\n",
            "Saved: /content/submission.csv\n"
          ]
        }
      ],
      "source": [
        "# Collect TEST files\n",
        "test_files = sorted(glob.glob(os.path.join(TEST_DIR, 'TEST_*.csv')))\n",
        "print('Found test files:', len(test_files))\n",
        "\n",
        "# Combine predictions\n",
        "all_preds = []\n",
        "for path in test_files:\n",
        "    # Pass the necessary indices to the prediction function\n",
        "    pred_df = predict_one_test(model, scaler, feat_cols, scaling_cols_indices, non_scaling_cols_indices, path)\n",
        "    all_preds.append(pred_df)\n",
        "full_pred_df = pd.concat(all_preds, ignore_index=True)\n",
        "\n",
        "# Convert to submission format\n",
        "sample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "submission = convert_to_submission_format(full_pred_df, sample_submission)\n",
        "\n",
        "# Save\n",
        "submission.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
        "print('Saved:', OUT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SntgA5eJXR-C",
      "metadata": {
        "id": "SntgA5eJXR-C"
      },
      "source": [
        "#Optuna parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5NR9-3-dXRDk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NR9-3-dXRDk",
        "outputId": "515296de-3cb3-4272-bd59-6ea09f5a7b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-23 15:35:42,615] A new study created in memory with name: no-name-66238b35-9d0d-4c1b-b1d8-f1ffac24f8fa\n",
            "[I 2025-08-23 15:39:19,036] Trial 0 finished with value: 26.384389419505432 and parameters: {'max_depth': 4, 'min_samples_leaf': 21, 'n_estimators': 1185, 'learning_rate': 0.3906222061594458, 'loss': 'linear'}. Best is trial 0 with value: 26.384389419505432.\n",
            "[I 2025-08-23 15:45:56,512] Trial 1 finished with value: 116.09273541238916 and parameters: {'max_depth': 4, 'min_samples_leaf': 5, 'n_estimators': 331, 'learning_rate': 0.3942612878325853, 'loss': 'square'}. Best is trial 0 with value: 26.384389419505432.\n",
            "[I 2025-08-23 15:49:02,368] Trial 2 finished with value: 15.735558227043224 and parameters: {'max_depth': 8, 'min_samples_leaf': 6, 'n_estimators': 1818, 'learning_rate': 0.30634118040344516, 'loss': 'linear'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 15:51:19,823] Trial 3 finished with value: 18.313978143453618 and parameters: {'max_depth': 5, 'min_samples_leaf': 27, 'n_estimators': 1783, 'learning_rate': 0.44497921137828295, 'loss': 'linear'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 16:10:53,450] Trial 4 finished with value: 143.2538377417587 and parameters: {'max_depth': 5, 'min_samples_leaf': 24, 'n_estimators': 910, 'learning_rate': 0.16076510417643827, 'loss': 'square'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 16:17:10,863] Trial 5 finished with value: 51.90573971137481 and parameters: {'max_depth': 3, 'min_samples_leaf': 8, 'n_estimators': 1155, 'learning_rate': 0.19077931834626888, 'loss': 'linear'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 16:20:34,655] Trial 6 finished with value: 66.83648247437544 and parameters: {'max_depth': 3, 'min_samples_leaf': 6, 'n_estimators': 1883, 'learning_rate': 0.4313213461269468, 'loss': 'linear'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 16:26:01,666] Trial 7 finished with value: 59.84465391486528 and parameters: {'max_depth': 3, 'min_samples_leaf': 19, 'n_estimators': 1567, 'learning_rate': 0.2701486968385809, 'loss': 'linear'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 16:51:35,258] Trial 8 finished with value: 142.12153820638574 and parameters: {'max_depth': 7, 'min_samples_leaf': 16, 'n_estimators': 1098, 'learning_rate': 0.1542962656521919, 'loss': 'square'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 17:00:07,635] Trial 9 finished with value: 23.997637086097626 and parameters: {'max_depth': 4, 'min_samples_leaf': 17, 'n_estimators': 1998, 'learning_rate': 0.14154072382078778, 'loss': 'linear'}. Best is trial 2 with value: 15.735558227043224.\n",
            "[I 2025-08-23 17:18:55,505] Trial 10 finished with value: 15.311477934412329 and parameters: {'max_depth': 9, 'min_samples_leaf': 11, 'n_estimators': 215, 'learning_rate': 0.052203396812208064, 'loss': 'square'}. Best is trial 10 with value: 15.311477934412329.\n",
            "[I 2025-08-23 17:29:28,684] Trial 11 finished with value: 15.308017067056682 and parameters: {'max_depth': 9, 'min_samples_leaf': 11, 'n_estimators': 90, 'learning_rate': 0.06697622449181495, 'loss': 'square'}. Best is trial 11 with value: 15.308017067056682.\n",
            "[I 2025-08-23 17:53:47,795] Trial 12 finished with value: 15.279699298235053 and parameters: {'max_depth': 9, 'min_samples_leaf': 11, 'n_estimators': 191, 'learning_rate': 0.0227700153996597, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 18:28:16,176] Trial 13 finished with value: 15.311054342621864 and parameters: {'max_depth': 9, 'min_samples_leaf': 12, 'n_estimators': 538, 'learning_rate': 0.03365037796641104, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 18:48:24,927] Trial 14 finished with value: 17.018189486813167 and parameters: {'max_depth': 7, 'min_samples_leaf': 12, 'n_estimators': 598, 'learning_rate': 0.07998069874478003, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 18:57:25,444] Trial 15 finished with value: 15.943225117716944 and parameters: {'max_depth': 7, 'min_samples_leaf': 3, 'n_estimators': 113, 'learning_rate': 0.08736184502091487, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 19:56:24,905] Trial 16 finished with value: 15.556960965465516 and parameters: {'max_depth': 8, 'min_samples_leaf': 10, 'n_estimators': 552, 'learning_rate': 0.011236266840248414, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 20:17:16,681] Trial 17 finished with value: 146.3978376751292 and parameters: {'max_depth': 9, 'min_samples_leaf': 16, 'n_estimators': 855, 'learning_rate': 0.21803878145500197, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 20:30:37,869] Trial 18 finished with value: 16.27211716539276 and parameters: {'max_depth': 8, 'min_samples_leaf': 30, 'n_estimators': 321, 'learning_rate': 0.10947942071865807, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 20:44:49,064] Trial 19 finished with value: 133.70560533630447 and parameters: {'max_depth': 6, 'min_samples_leaf': 14, 'n_estimators': 722, 'learning_rate': 0.31446869078474493, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 20:51:55,211] Trial 20 finished with value: 16.593991187449948 and parameters: {'max_depth': 6, 'min_samples_leaf': 2, 'n_estimators': 114, 'learning_rate': 0.11417551597427794, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 21:44:13,213] Trial 21 finished with value: 15.317236028440524 and parameters: {'max_depth': 9, 'min_samples_leaf': 13, 'n_estimators': 443, 'learning_rate': 0.01303681994682905, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 22:05:32,045] Trial 22 finished with value: 15.331377519033648 and parameters: {'max_depth': 9, 'min_samples_leaf': 9, 'n_estimators': 414, 'learning_rate': 0.06190152950399321, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 22:16:31,471] Trial 23 finished with value: 15.498741025422955 and parameters: {'max_depth': 8, 'min_samples_leaf': 14, 'n_estimators': 94, 'learning_rate': 0.040357113978370635, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 22:41:10,677] Trial 24 finished with value: 15.338855721131507 and parameters: {'max_depth': 9, 'min_samples_leaf': 8, 'n_estimators': 276, 'learning_rate': 0.03899812282498056, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 22:59:27,319] Trial 25 finished with value: 26.388765977823425 and parameters: {'max_depth': 8, 'min_samples_leaf': 19, 'n_estimators': 591, 'learning_rate': 0.10818744184782643, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 23:30:27,633] Trial 26 finished with value: 155.77333371379316 and parameters: {'max_depth': 7, 'min_samples_leaf': 11, 'n_estimators': 1381, 'learning_rate': 0.2026736789189583, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n",
            "[I 2025-08-23 23:43:50,787] Trial 27 finished with value: 267.5321135636229 and parameters: {'max_depth': 2, 'min_samples_leaf': 14, 'n_estimators': 754, 'learning_rate': 0.49286239090298456, 'loss': 'square'}. Best is trial 12 with value: 15.279699298235053.\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "if '영업일자' in train.columns and 'date' not in train.columns:\n",
        "    train = train.rename(columns={'영업일자':'date'})\n",
        "train['date'] = pd.to_datetime(train['date'])\n",
        "\n",
        "# Build supervised learning dataset\n",
        "X, y, ref_dates, feat_cols = build_supervised(train)\n",
        "trn_idx, val_idx = time_based_split(ref_dates, valid_days=VALID_DAYS)\n",
        "Xtr, ytr = X[trn_idx], y[trn_idx]\n",
        "Xva, yva = X[val_idx], y[val_idx]\n",
        "\n",
        "# --- Selective Scaling ---\n",
        "non_scaling_cols = ['season_cos', 'season_sin']\n",
        "scaling_cols = [c for c in feat_cols if c not in non_scaling_cols]\n",
        "\n",
        "# Get indices for splitting the arrays\n",
        "scaling_cols_indices = [feat_cols.index(c) for c in scaling_cols]\n",
        "non_scaling_cols_indices = [feat_cols.index(c) for c in non_scaling_cols]\n",
        "\n",
        "# Separate data for scaling\n",
        "Xtr_to_scale = Xtr[:, scaling_cols_indices]\n",
        "Xtr_no_scale = Xtr[:, non_scaling_cols_indices]\n",
        "Xva_to_scale = Xva[:, scaling_cols_indices]\n",
        "Xva_no_scale = Xva[:, non_scaling_cols_indices]\n",
        "\n",
        "# Fit scaler ONLY on the columns to be scaled from the training set\n",
        "scaler = MinMaxScaler().fit(Xtr_to_scale)\n",
        "\n",
        "# Transform and concatenate\n",
        "Xtr_scaled = scaler.transform(Xtr_to_scale)\n",
        "Xtr_s = np.concatenate([Xtr_scaled, Xtr_no_scale], axis=1)\n",
        "\n",
        "Xva_scaled = scaler.transform(Xva_to_scale) if len(Xva) > 0 else None\n",
        "Xva_s = np.concatenate([Xva_scaled, Xva_no_scale], axis=1) if Xva_scaled is not None else None\n",
        "\n",
        "def objective(trial):\n",
        "  max_depth = trial.suggest_int(\"max_depth\", 2, 9)\n",
        "  min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 30)\n",
        "##############################adaboost parameters###########################\n",
        "  n_estimators = trial.suggest_int(\"n_estimators\", 70, 2000)\n",
        "  learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.5)\n",
        "  loss = trial.suggest_categorical(\"loss\", ['linear', 'square'])\n",
        "\n",
        "  base_estimator = DecisionTreeRegressor(\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "  model = MultiOutputRegressor(\n",
        "      AdaBoostRegressor(\n",
        "          estimator = base_estimator,\n",
        "          n_estimators = n_estimators,\n",
        "          learning_rate = learning_rate,\n",
        "          loss = loss,\n",
        "          random_state = 42\n",
        "      )\n",
        "  )\n",
        "  model.fit(Xtr_s, ytr)\n",
        "\n",
        "\n",
        "  y_pred = model.predict(Xva_s)\n",
        "\n",
        "\n",
        "  score = root_mean_squared_error(yva, y_pred)\n",
        "\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=40)\n",
        "\n",
        "print(\"Best RMSE:\", study.best_value)\n",
        "print(\"Best Params:\", study.best_trial.params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}